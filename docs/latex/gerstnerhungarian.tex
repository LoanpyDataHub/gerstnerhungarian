%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{gerstnerhungarian}
\date{May 25, 2023}
\release{2.0}
\author{Viktor Martinović}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{Home}
\label{\detokenize{home:home}}\label{\detokenize{home::doc}}

\section{CLDF dataset derived from a preprint of ‘Új magyar etimológiai szótár’ {[}New Hungarian Etymological Dictionary{]} by Károly Gerstner (ed.)}
\label{\detokenize{home:cldf-dataset-derived-from-a-preprint-of-uj-magyar-etimologiai-szotar-new-hungarian-etymological-dictionary-by-karoly-gerstner-ed}}

\subsection{How to cite}
\label{\detokenize{home:how-to-cite}}
\sphinxAtStartPar
If you use these data please cite
\begin{itemize}
\item {} 
\sphinxAtStartPar
the original source: Gerstner, Károly (ed.) (2011\sphinxhyphen{}2023).
Új magyar Etimológiai Szótár.
Hungarian Academy of Sciences, Budapest. \sphinxurl{http://uesz.nytud.hu/}.

\item {} 
\sphinxAtStartPar
the derived dataset using the DOI of the \sphinxhref{https://github.com/LoanpyDataHub/gerstnerhungarian/releases}{particular released
version}
you were using

\end{itemize}


\subsection{Description}
\label{\detokenize{home:description}}
\sphinxAtStartPar
This dataset is licensed under a CC\sphinxhyphen{}BY 4.0 license

\sphinxAtStartPar
Conceptlists in Concepticon:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://concepticon.clld.org/contributions/Dellert-2018-1016}{Dellert\sphinxhyphen{}2018\sphinxhyphen{}1016}

\end{itemize}


\subsection{Notes}
\label{\detokenize{home:notes}}
\sphinxAtStartPar
\sphinxhref{https://creativecommons.org/licenses/by/4.0/}{\sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/554312bbffabcb804cf4f8c50b1b75a140d3bd2b/by}.svg}} \sphinxhref{https://dl.circleci.com/status-badge/redirect/gh/LoanpyDataHub/gerstnerhungarian/tree/main}{\sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/5221f732c98234d1f31dd595c54f19217d01ec6a/main}.svg}} \sphinxhref{https://gerstnerhungarian.readthedocs.io/en/latest/?badge=latest}{\sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/941b25609704529107ac2a098aef56238d782e60/04afe4fcf94364fa02d681ebc3ad2f4dd1402430}.svg}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tools used to create parts of the orthographic profile:
\sphinxurl{https://digling.org/calc/profiler/}

\item {} 
\sphinxAtStartPar
Skeleton for the Hungarian orthographic profile comes from:
\sphinxurl{https://github.com/dmort27/epitran/blob/master/epitran/data/map/hun-Latn.csv}

\end{itemize}


\subsection{Statistics}
\label{\detokenize{home:statistics}}
\sphinxAtStartPar
\sphinxhref{https://github.com/martino-vic/gerstnerhungarian/actions?query=workflow\%3ACLDF-validation}{\sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/4a378d74fa8541e029a8e727f0722e6159b8f949/badge}.svg}} \sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/d44641bc24451db56ba1c2f56d656871246a1945/Glottolog-100%25-brightgreen}.svg} \sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/76c362aa69c478e0e38fe53aa1207f4c4e0247c2/3984c76090ee859c11f6da2984a8848fac1f0c3e}.svg} \sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/8610b4fe2e7a97bed23df74a39cd30feda0b8612/Source-100%25-brightgreen}.svg}
\sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/54c8ae34ac3ec41286b604aef9bcbad15a74f3de/BIPA-100%25-brightgreen}.svg} \sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/64600fa1cdb6b54d89ce1e5ebac95262ca62a8fc/b8fdf0ffe8e33ddbdc635aa3b469289f77c84efb}.svg} \sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/be037cb372fe82ccb3e3ab6c2ab1eb62ef77e6b0/7690bf4d2ca2c181b71fba959ee8e1f1f318b06e}.svg} \sphinxhref{https://pypi.org/project/spacy/}{\sphinxincludegraphics{{/home/viktor/Documents/GitHub/gerstnerhungarian/docs/doctrees/images/2d081927145272de8a9ed568341618987dcea5d3/SpaCy-v3.5}.1-blue}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Varieties:} 1

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Concepts:} 890

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Lexemes:} 4,084

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sources:} 1

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Synonymy:} 4.59

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Invalid lexemes:} 0

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tokens:} 19,042

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Segments:} 60 (0 BIPA errors, 0 CLTS sound class errors, 60 CLTS
modified)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Inventory size (avg):} 60.00

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Senses:} 43,659

\end{itemize}


\section{Contributors}
\label{\detokenize{home:contributors}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Name
&\sphinxstyletheadfamily 
\sphinxAtStartPar
GitHub user
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Role
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Károly Gerstner
&&
\sphinxAtStartPar
ÚESz {[}New
Hungarian
Etymological
Dictionary{]},
continuation of
EWUng
&
\sphinxAtStartPar
Editor
\\
\sphinxhline
\sphinxAtStartPar
Loránd Benkő
&&
\sphinxAtStartPar
EWUng
{[}Etymological
Dictionary of
Hungarian{]},
continuation of
TESz
&
\sphinxAtStartPar
Editor
\\
\sphinxhline
\sphinxAtStartPar
Loránd Benkő
&&
\sphinxAtStartPar
TESz {[}A
Historic
al\sphinxhyphen{}Etymological
Dictionary of
Hungarian{]}
&
\sphinxAtStartPar
Editor
\\
\sphinxhline
\sphinxAtStartPar
Viktor
Martinović
&
\sphinxAtStartPar
@martino\sphinxhyphen{}vic
&
\sphinxAtStartPar
CLDF conversion
&
\sphinxAtStartPar
Other
\\
\sphinxhline
\sphinxAtStartPar
Johann\sphinxhyphen{}Mattis
List
&
\sphinxAtStartPar
@LinguList
&
\sphinxAtStartPar
CLDF conversion
&
\sphinxAtStartPar
Other
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{CLDF Datasets}
\label{\detokenize{home:cldf-datasets}}
\sphinxAtStartPar
The following CLDF datasets are available in \sphinxhref{https://github.com/LoanpyDataHub/gerstnerhungarian/tree/main/cldf}{cldf}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
CLDF
\sphinxhref{https://github.com/cldf/cldf/tree/master/modules/Wordlist}{Wordlist}
at \sphinxhref{https://github.com/LoanpyDataHub/gerstnerhungarian/blob/main/cldf/cldf-metadata.json}{cldf/cldf\sphinxhyphen{}metadata.json}

\item {} 
\sphinxAtStartPar
CLDF
\sphinxhref{https://github.com/cldf/cldf/tree/master/modules/Dictionary}{Dictionary}
at \sphinxhref{https://github.com/LoanpyDataHub/gerstnerhungarian/blob/main/cldf/Dictionary-metadata.json}{cldf/Dictionary\sphinxhyphen{}metadata.json}

\end{itemize}

\sphinxstepscope


\chapter{TL;DR}
\label{\detokenize{TL;DR:id1}}\label{\detokenize{TL;DR::doc}}\phantomsection\label{\detokenize{TL;DR:module-gerstnerhungariancommands.__init__}}\index{module@\spxentry{module}!gerstnerhungariancommands.\_\_init\_\_@\spxentry{gerstnerhungariancommands.\_\_init\_\_}}\index{gerstnerhungariancommands.\_\_init\_\_@\spxentry{gerstnerhungariancommands.\_\_init\_\_}!module@\spxentry{module}}
\sphinxAtStartPar
Open terminal in folder containing GitHub projects
or in a newly created folder and run the commands below to…

\sphinxAtStartPar
…create the contents of the cldf\sphinxhyphen{}folder (downloadsize: 1.5GB+):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv\PYG{+w}{ }\PYG{o}{\PYGZam{}\PYGZam{}}\PYG{+w}{ }\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate

git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/martino\PYGZhy{}vic/gerstnerhungarian.git
mkdir\PYG{+w}{ }concepticon
\PYG{n+nb}{cd}\PYG{+w}{ }concepticon
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/concepticon/concepticon\PYGZhy{}data.git
\PYG{n+nb}{cd}\PYG{+w}{ }..
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/glottolog/glottolog.git
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/cldf\PYGZhy{}clts/clts.git

pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}e\PYG{+w}{ }gerstnerhungarian
pip\PYG{+w}{ }install\PYG{+w}{ }loanpy
pip\PYG{+w}{ }install\PYG{+w}{ }pytest\PYGZhy{}cldf

python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }spacy\PYG{+w}{ }download\PYG{+w}{ }de\PYGZus{}core\PYGZus{}news\PYGZus{}lg

\PYG{n+nb}{cd}\PYG{+w}{ }gerstnerhungarian
cldfbench\PYG{+w}{ }lexibank.makecldf\PYG{+w}{ }lexibank\PYGZus{}gerstnerhungarian.py\PYG{+w}{  }\PYGZhy{}\PYGZhy{}concepticon\PYGZhy{}version\PYG{o}{=}v2.5.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYGZhy{}version\PYG{o}{=}v4.5\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYGZhy{}version\PYG{o}{=}v2.2.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}concepticon\PYG{o}{=}../concepticon/concepticon\PYGZhy{}data\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYG{o}{=}../glottolog\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYG{o}{=}../clts

cldfbench\PYG{+w}{ }gerstnerhungarian.update\PYGZus{}readme
\end{sphinxVerbatim}

\sphinxAtStartPar
…to filter word lists according to different criteria:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }gerstnerhungarian.map

cldfbench\PYG{+w}{ }gerstnerhungarian.filter\PYG{+w}{ }\PYGZhy{}y\PYG{+w}{ }\PYG{l+m}{1600}\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }SlavicTurkic\PYG{+w}{ }\PYGZhy{}a
\end{sphinxVerbatim}

\sphinxAtStartPar
…to test whether the converted data conforms to the CLDF standard:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cldf\PYGZhy{}metadata\PYG{o}{=}cldf/cldf\PYGZhy{}metadata.json\PYG{+w}{ }test.py
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Part 1: Create CLDF}
\label{\detokenize{mkcldf:part-1-create-cldf}}\label{\detokenize{mkcldf::doc}}
\sphinxAtStartPar
The following six steps will guide you through the process of
converting raw language data to CLDF. Each step can be found in the
\sphinxhref{https://app.circleci.com/pipelines/github/LoanpyDataHub/gerstnerhungarian}{continuous integration workflow}
as well. The data we are converting comes from
the \sphinxhref{https://uesz.nytud.hu/index.html}{New Hungarian Etymological Dictionary} (Gerstner 2022),
which contains modern \sphinxhref{https://glottolog.org/resource/languoid/id/hung1274}{Hungarian} words as headwords,
together with their
etymological source language and year of first appearance in a written source.
The oldest layer is inherited from Proto\sphinxhyphen{}Uralic, followed chronologically by
Proto\sphinxhyphen{}Finno\sphinxhyphen{}Ugric and
Proto\sphinxhyphen{}Ugric. The next oldest layer is borrowed from a Turkic language called
West Old Turkic, or \sphinxhref{https://glottolog.org/resource/languoid/id/bolg1249}{Proto\sphinxhyphen{}Bolgar}. The raw data in this
repository contains
only a small fraction of the contents of the dictionary.
If you are passionate about Hungarian etymologies and want to contribute
to this repository, check out the \sphinxhref{https://github.com/martino-vic/gerstnerhungarian/blob/main/CONTRIBUTING.md}{guidelines}
and let’s get in touch!


\section{Step 1: Activate virtual environment and clone the repository}
\label{\detokenize{mkcldf:step-1-activate-virtual-environment-and-clone-the-repository}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv\PYG{+w}{ }\PYG{o}{\PYGZam{}\PYGZam{}}\PYG{+w}{ }\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/martino\PYGZhy{}vic/gerstnerhungarian.git
\end{sphinxVerbatim}

\sphinxAtStartPar
To deactivate the virtual environment run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
deactivate
\end{sphinxVerbatim}

\sphinxAtStartPar
All steps in this documentation should be carried out in a virtual
environment. Originally, the skeleton of the repository was created using this
command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }new
\end{sphinxVerbatim}

\sphinxAtStartPar
More on this can be read in the
\sphinxhref{https://github.com/cldf/cldfbench/blob/master/doc/tutorial.md}{cldfbench tutorial}.
Next, open the file \sphinxcode{\sphinxupquote{metadata.json}} and manually add the line
\sphinxcode{\sphinxupquote{"conceptlist": "Dellert\sphinxhyphen{}2018\sphinxhyphen{}1016"}} to it. This will give access to the
concept list used by the \sphinxhref{http://www.northeuralex.org/}{NorthEuraLex}
project. There, this concept list was used for comparison of Uralic languages,
Hungarian among others, and was therefore deemed adequate
to filter the raw input data of this repository according to it.


\section{Step 2: Clone reference catalogues}
\label{\detokenize{mkcldf:step-2-clone-reference-catalogues}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://glottolog.org/}{Glottolog} (Hammarström et al. 2022)
to reference the languages in the repo.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://concepticon.clld.org/}{Concepticon} (List et al. 2023) to
reference concepts.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://clts.clld.org/}{CLTS} (List et al. 2021) to reference IPA
characters

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
The following repositories will take over 1GB in disk\sphinxhyphen{}space. If you skip
cloning them,
add a \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}dev}} flag to the command running the lexibank script in step 5.
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir\PYG{+w}{ }concepticon
\PYG{n+nb}{cd}\PYG{+w}{ }concepticon
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/concepticon/concepticon\PYGZhy{}data.git
\PYG{n+nb}{cd}\PYG{+w}{ }..
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/glottolog/glottolog.git
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/cldf\PYGZhy{}clts/clts.git
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/martino\PYGZhy{}vic/loanpy.git
\end{sphinxVerbatim}


\section{Step 3: Install commands, download wordvectors, create orthographic profile}
\label{\detokenize{mkcldf:step-3-install-commands-download-wordvectors-create-orthographic-profile}}
\sphinxAtStartPar
First, install \sphinxhref{https://loanpy.readthedocs.io/en/latest/home.html}{loanpy}
(Martinović 2023):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip\PYG{+w}{ }install\PYG{+w}{ }loanpy
\end{sphinxVerbatim}

\sphinxAtStartPar
Then install this repository:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}e\PYG{+w}{ }gerstnerhungarian
\end{sphinxVerbatim}

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{\sphinxhyphen{}e}} flag will install all necessary dependencies in development mode.
I.e. if you modify any code in those repositories, changes will apply
immediately.
Installing this package will also install all its dependencies,
which are specified in the \sphinxcode{\sphinxupquote{setup.py}} file. One of those
dependencies is
\sphinxhref{https://pypi.org/project/spacy/}{Spacy}. Spacy offers pre\sphinxhyphen{}trained
wordvector models for German. Since NLP is a rapidly changing field, it is
adviced
to always download the most up\sphinxhyphen{}to\sphinxhyphen{}date vectors from the most up\sphinxhyphen{}to\sphinxhyphen{}date
packages. The architecture allows you to replace Spacy with any library
or function of choice. We will be using a 500MB model for the current
use case. If this should be too large, you can browse smaller German models at
\sphinxurl{https://spacy.io/models/de/}. To download the 500MB file run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }spacy\PYG{+w}{ }download\PYG{+w}{ }de\PYGZus{}core\PYGZus{}news\PYGZus{}lg
\end{sphinxVerbatim}

\sphinxAtStartPar
After downloading the word\sphinxhyphen{}vectors to your system, let’s create the
orthographic profiles: The process of creating an orthographic profile for
Hungarian was described in \sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/mkcldf.html\#step-4-create-ipa-transcriptions}{step 4 of the documentation of the
ronataswestoldturkic repository}
. Since the current repository only covers one language, there is no
need for a folder \sphinxcode{\sphinxupquote{orthography}} with multiple profiles. A single file named
\sphinxcode{\sphinxupquote{orthography.tsv}} is enough. The file itself was taken from
\sphinxcode{\sphinxupquote{ronataswestoldturkic/etc/orthography/H.tsv}}.


\section{Step 4: Run lexibank script}
\label{\detokenize{mkcldf:step-4-run-lexibank-script}}
\sphinxAtStartPar
This script combines files from the \sphinxcode{\sphinxupquote{raw}} and \sphinxcode{\sphinxupquote{etc}} folders
and populates the folder \sphinxtitleref{cldf}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }lexibank.makecldf\PYG{+w}{ }lexibank\PYGZus{}ronataswestoldturkic.py\PYG{+w}{  }\PYGZhy{}\PYGZhy{}concepticon\PYGZhy{}version\PYG{o}{=}v2.5.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYGZhy{}version\PYG{o}{=}v4.5\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYGZhy{}version\PYG{o}{=}v2.2.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}concepticon\PYG{o}{=}../concepticon/concepticon\PYGZhy{}data\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYG{o}{=}../glottolog\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYG{o}{=}../clts
cldfbench\PYG{+w}{ }gerstnerhungarian.update\PYGZus{}readme
\end{sphinxVerbatim}

\sphinxAtStartPar
The first line of this shell script invokes \sphinxhref{https://pure.mpg.de/rest/items/item\_3259068/component/file\_3261838/content}{cldfbench},
a workbench for creating and managing CLDF datasets. The first three
flags (“dash dash”) specify the versions of the reference catalogues. This is
important,
since wrong versions can lead to mismatches in the references and may
obstruct the CLDF\sphinxhyphen{}conversion.
The last three flags specify the location of the reference
catalogues. Those flags were added with increased replicability and
maintainability in mind (Even though there is an alternative, namely editing
the \sphinxstyleemphasis{catalog.ini} file with a text editor like \sphinxstyleemphasis{nano}, as showed in
\sphinxhref{https://calc.hypotheses.org/2225}{this blog post}).
The second line is a custom command that updates the readme by adding some
custom badges and statistics.

\sphinxAtStartPar
Below is a detailed description of what the lexibank script does.
See also the tutorial at \sphinxurl{https://calc.hypotheses.org/3318}, which has many
similarities.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pathlib}
\PYG{k+kn}{import} \PYG{n+nn}{re}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}
\PYG{k+kn}{from} \PYG{n+nn}{functools} \PYG{k+kn}{import} \PYG{n}{lru\PYGZus{}cache}
\end{sphinxVerbatim}

\sphinxAtStartPar
First, we import four inbuilt Python\sphinxhyphen{}libraries.
\begin{itemize}
\item {} 
\sphinxAtStartPar
The \sphinxhref{https://docs.python.org/3/library/pathlib.html}{pathlib} library
will be used to define the parent directory of the dataset, relative to
which all other files will be read and written.

\item {} 
\sphinxAtStartPar
The regular expression library \sphinxhref{https://docs.python.org/3/library/re.html}{re}
will be used for data cleaning with the \sphinxhref{https://docs.python.org/3/library/re.html\#re.sub}{re.sub} function and for
splitting strings with \sphinxhref{https://docs.python.org/3/library/re.html\#re.Pattern.split}{re.split}.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://docs.python.org/3/library/collections.html\#collections.defaultdict}{defaultdict}
defines a data type to which missing dictionary keys automatically default.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://docs.python.org/3/library/functools.html\#functools.lru\_cache}{lru\_cache}
will help to speed up looking up word\sphinxhyphen{}vectors, since the same words are being
looked up often.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{attr}
\PYG{k+kn}{import} \PYG{n+nn}{pylexibank}
\PYG{k+kn}{import} \PYG{n+nn}{spacy}
\PYG{k+kn}{from} \PYG{n+nn}{cldfbench} \PYG{k+kn}{import} \PYG{n}{CLDFSpec}
\PYG{k+kn}{from} \PYG{n+nn}{clldutils}\PYG{n+nn}{.}\PYG{n+nn}{misc} \PYG{k+kn}{import} \PYG{n}{slug}
\PYG{k+kn}{from} \PYG{n+nn}{epitran} \PYG{k+kn}{import} \PYG{n}{Epitran}
\PYG{k+kn}{from} \PYG{n+nn}{lingpy}\PYG{n+nn}{.}\PYG{n+nn}{sequence}\PYG{n+nn}{.}\PYG{n+nn}{sound\PYGZus{}classes} \PYG{k+kn}{import} \PYG{n}{ipa2tokens}
\PYG{k+kn}{from} \PYG{n+nn}{loanpy}\PYG{n+nn}{.}\PYG{n+nn}{scapplier} \PYG{k+kn}{import} \PYG{n}{Adrc}
\PYG{k+kn}{from} \PYG{n+nn}{pylexibank} \PYG{k+kn}{import} \PYG{n}{Dataset} \PYG{k}{as} \PYG{n}{BaseDataset}\PYG{p}{,} \PYG{n}{FormSpec}\PYG{p}{,} \PYG{n}{Lexeme}
\PYG{k+kn}{from} \PYG{n+nn}{tqdm} \PYG{k+kn}{import} \PYG{n}{tqdm}
\end{sphinxVerbatim}

\sphinxAtStartPar
Then, we import functionalities from various third\sphinxhyphen{}party libraries.
These were installed when running \sphinxcode{\sphinxupquote{pip install \sphinxhyphen{}e gerstnerhungarian}}
eariler.
\begin{itemize}
\item {} 
\sphinxAtStartPar
With the \sphinxhref{https://www.attrs.org/en/stable/index.html}{attrs} library
we will create the custom language class with custom columns in the output
file \sphinxcode{\sphinxupquote{cldf/forms.csv}}.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://pypi.org/project/spacy/}{Spacy} will be used to check the word
vector coverage of the meanings associated with each headword.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://clldutils.readthedocs.io/en/latest/misc.html\#clldutils.misc.slug}{slug}
function from the clldutils library will be used to format some IDs.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://pypi.org/project/epitran/}{epitran} library will be used to
transcribe words from Hungarian orthography to IPA.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://lingpy.readthedocs.io/en/latest/reference/lingpy.sequence.html\#lingpy.sequence.sound\_classes.ipa2tokens}{ipa2tokens}
function from the lingpy library will be used to tokenise ipa\sphinxhyphen{}strings.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scapplier.Adrc}{Adrc}
class from loanpy will be used to predict historical
reconstructions based on sound changes that were extracted in the
\sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/home.html}{ronataswestoldturkic}
repository.

\item {} 
\sphinxAtStartPar
The classes from the \sphinxhref{https://pypi.org/project/pylexibank/}{pylexibank}
library are all related to specifying the output format. \sphinxcode{\sphinxupquote{Dataset}} for
example
loads the default data format, \sphinxcode{\sphinxupquote{Lexeme}} will be used to customise it, and
\sphinxcode{\sphinxupquote{FormSpec}} will be used to document the cleaning of the raw data.

\item {} 
\sphinxAtStartPar
The tqdm library is used to create a progressbar in our command line.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HOWMANY} \PYG{o}{=} \PYG{l+m+mi}{700}
\PYG{n}{TRIMLIST} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dozik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kodik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kedik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kozik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ködik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{odik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ődik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ozik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{edik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ödik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ázik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ezik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{edik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ödik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ődik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ozik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ik\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here, we are defining two static parameters. One is the number of predictions
we want to make per Hungarian word. The optimum was calculated in \sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/mkloanpy.html\#step-5-evaluate-vertical-and-horizontal-sound-correspondences}{Step 5}
and visualised in \sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/mkloanpy.html\#step-6-plot-the-evaluations}{Step 6}
of the ronataswestoldturkic repository. The other is a list of regular
expressions that will later be used to remove suffixes in Hungarian verbs that
have a different etymology than the rest of the verb they belong to.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{REP} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{†×∆\PYGZhy{}\(\sp{\text{1}}\)\(\sp{\text{2}}\)\(\sp{\text{3}}\)\(\sp{\text{4}}\)’}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{nlp} \PYG{o}{=} \PYG{n}{spacy}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{de\PYGZus{}core\PYGZus{}news\PYGZus{}lg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{rc} \PYG{o}{=} \PYG{n}{Adrc}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{etc/H2EAHsc.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{orth2ipa} \PYG{o}{=} \PYG{n}{Epitran}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hun\PYGZhy{}Latn}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transliterate}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this block we are defining some global variables that we will need later.
The variable REP stands for ‘replacements’ and will be used to create
the column “forms” from the column “values”, where replacements are hard\sphinxhyphen{}coded.
There is only one simple replacement rule, which is expressed as a list
comprehension, namely that characters “†×∆\sphinxhyphen{}\(\sp{\text{1}}\)\(\sp{\text{2}}\)\(\sp{\text{3}}\)\(\sp{\text{4}}\)’” are deleted in each
word.

\sphinxAtStartPar
Next, we are loading the word\sphinxhyphen{}vectors that we have downloaded in step 3.
\sphinxcode{\sphinxupquote{rc}} is an instance of
loanpy’s \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scapplier.Adrc}{Adrc}
class and “etc/H2EAHsc.json” is the sound correspondence file
we have generated in \sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/mkloanpy.html\#step-3-mine-vertical-and-horizontal-sound-correspondences}{Part 3, step 3 of the ronataswestoldturkic
repository}.
The file itself has been directly copied from
\sphinxcode{\sphinxupquote{ronataswestoldturkic/loanpy/H2EAHsc.json}}. This is the information based
on which we will reconstruct hypothetical Early Ancient Hungarian forms.
Lastly, \sphinxcode{\sphinxupquote{orth2ipa}} is a function that transcribes strings in Hungarian
orthography to IPA with the help of the \sphinxhref{https://pypi.org/project/epitran/}{epitran} package.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@attr}\PYG{o}{.}\PYG{n}{s}
\PYG{k}{class} \PYG{n+nc}{CustomLexeme}\PYG{p}{(}\PYG{n}{Lexeme}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{Meaning} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
    \PYG{n}{Sense\PYGZus{}ID} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
    \PYG{n}{Entry\PYGZus{}ID} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are defining three custom columns that are not included by default,
using \sphinxhref{https://www.attrs.org/en/stable/api-attr.html\#attr.ib}{attr.ib}
and the Lexeme class that we have imported earlier.
The column \sphinxcode{\sphinxupquote{Meaning}} comes directly from the raw file and contains a
comma\sphinxhyphen{}space
separated list of translations into English that we will call “senses”.
\sphinxcode{\sphinxupquote{Sense\_ID}} is a foreign key that points to one of the senses in the
\sphinxcode{\sphinxupquote{senses.csv}} table and \sphinxcode{\sphinxupquote{Entry\_ID}} is a foreign key that points to the
corresponding row in \sphinxcode{\sphinxupquote{entries.csv}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{clean1}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{re}\PYG{o}{.}\PYG{n}{sub}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[†×∆}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{\PYGZhy{}\(\sp{\text{1}}\)\(\sp{\text{2}}\)\(\sp{\text{3}}\)\(\sp{\text{4}}\)’ ]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{word}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{clean}\PYG{p}{(}\PYG{n}{text}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{text} \PYG{o}{=} \PYG{n}{re}\PYG{o}{.}\PYG{n}{sub}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[〈〉:;!,.?\PYGZhy{}]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{text}\PYG{p}{)}
    \PYG{n}{text} \PYG{o}{=} \PYG{n}{re}\PYG{o}{.}\PYG{n}{sub}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{s+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{text}\PYG{p}{)}
    \PYG{n}{text} \PYG{o}{=} \PYG{n}{text}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{text}
\end{sphinxVerbatim}

\sphinxAtStartPar
These two functions will be used later in the script to remove some
characters from strings with the help of the \sphinxhref{https://docs.python.org/3/library/re.html}{re} library.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{seg\PYGZus{}ipa}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{word} \PYG{o}{=} \PYG{n}{clean1}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}
    \PYG{n}{word} \PYG{o}{=} \PYG{n}{orth2ipa}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}
    \PYG{n}{word} \PYG{o}{=} \PYG{n}{ipa2tokens}\PYG{p}{(}\PYG{n}{word}\PYG{p}{,} \PYG{n}{merge\PYGZus{}vowels}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{merge\PYGZus{}geminates}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{word}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This function will be used to populate the column \sphinxcode{\sphinxupquote{Segments}} in
\sphinxcode{\sphinxupquote{cldf/entries.csv}}. It cleans, ipa\sphinxhyphen{}transcribes, and segments
an input string and returns it as a space\sphinxhyphen{}separated string.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{trim}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{word} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{antik}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bolsevik}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{word}
    \PYG{k}{return} \PYG{n}{re}\PYG{o}{.}\PYG{n}{sub}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{|}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{TRIMLIST}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{clean1}\PYG{p}{(}\PYG{n}{word}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This function uses the static list of suffixes defined earlier and cuts
them off an input string.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@lru\PYGZus{}cache}\PYG{p}{(}\PYG{n}{maxsize}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{filter\PYGZus{}vectors}\PYG{p}{(}\PYG{n}{meaning}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{clean\PYGZus{}mean} \PYG{o}{=} \PYG{n}{clean}\PYG{p}{(}\PYG{n}{meaning}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{clean\PYGZus{}mean} \PYG{k}{if} \PYG{n}{nlp}\PYG{p}{(}\PYG{n}{clean\PYGZus{}mean}\PYG{p}{)}\PYG{o}{.}\PYG{n}{has\PYGZus{}vector} \PYG{k}{else} \PYG{k+kc}{None}
\end{sphinxVerbatim}

\sphinxAtStartPar
This function will be used when populating the column \sphinxcode{\sphinxupquote{Spacy}} in
\sphinxcode{\sphinxupquote{cldf/senses.csv}}. It takes a string as input, which can be a word
or a phrase. It then checks whether spacy’s word\sphinxhyphen{}vector model contains a
vector for the cleaned input. If yes, it returns the cleaned input, if not
it returns None, which translates to a blank row in our column.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Dataset}\PYG{p}{(}\PYG{n}{BaseDataset}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{dir} \PYG{o}{=} \PYG{n}{pathlib}\PYG{o}{.}\PYG{n}{Path}\PYG{p}{(}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{parent}
    \PYG{n+nb}{id} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gerstnerhungarian}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n}{form\PYGZus{}spec} \PYG{o}{=} \PYG{n}{FormSpec}\PYG{p}{(}\PYG{n}{separators}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{first\PYGZus{}form\PYGZus{}only}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                         \PYG{n}{replacements}\PYG{o}{=}\PYG{n}{REP}\PYG{p}{)}
    \PYG{n}{lexeme\PYGZus{}class} \PYG{o}{=} \PYG{n}{CustomLexeme}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we define a class and inherit the default format \sphinxcode{\sphinxupquote{BaseDataset}} that we
have imported in the beginning. \sphinxcode{\sphinxupquote{dir}} is the working directory and is
defined with the help of \sphinxcode{\sphinxupquote{pathlib}} that we have imported in the beginning.
\sphinxcode{\sphinxupquote{id}} is the name of the repository. In \sphinxcode{\sphinxupquote{lexeme\_class}} we are plugging in
the custom columns that we have created earlier. In \sphinxcode{\sphinxupquote{form\_spec}} we are
plugging in the data\sphinxhyphen{}cleaning rules that were read into the \sphinxcode{\sphinxupquote{REP}} variable
earlier, using the \sphinxcode{\sphinxupquote{FormSpec}} class we have imported in the beginning.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cmd\PYGZus{}makecldf}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{args}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
This function is being run when summoning the lexibank script from the command
line. It converts the data from folders \sphinxcode{\sphinxupquote{raw}} and \sphinxcode{\sphinxupquote{etc}} to standardised
CLDF data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{senses} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{)}
\PYG{n}{idxs} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{form2idx} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{raw\PYGZus{}dir}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gerstner\PYGZhy{}2016\PYGZhy{}10176.tsv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dicts}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sense}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{fidx} \PYG{o}{=} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{idx}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n}{slug}\PYG{p}{(}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{form}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{idxs}\PYG{p}{[}\PYG{n}{fidx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{row}
        \PYG{k}{for} \PYG{n}{sense} \PYG{o+ow}{in} \PYG{n}{re}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[,;]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sense}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{if} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{form}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)} \PYG{o+ow}{and} \PYG{n}{sense}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{senses}\PYG{p}{[}\PYG{n}{slug}\PYG{p}{(}\PYG{n}{sense}\PYG{p}{,} \PYG{n}{lowercase}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{n}{fidx}\PYG{p}{,} \PYG{n}{sense}\PYG{p}{)}\PYG{p}{]}
                \PYG{n}{form2idx}\PYG{p}{[}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{form}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sense}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{fidx}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are populating three dictionaries for later use: \sphinxcode{\sphinxupquote{senses}}
\sphinxcode{\sphinxupquote{idxs}} and \sphinxcode{\sphinxupquote{form2idx}}. The first will be used to create the table
\sphinxcode{\sphinxupquote{senses.csv}}, the second to create \sphinxcode{\sphinxupquote{cldf/entries.csv}} and the third
to create the foreign keys in column \sphinxcode{\sphinxupquote{Entry\_ID}} in \sphinxcode{\sphinxupquote{cldf/forms.csv}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cldf\PYGZus{}writer}\PYG{p}{(}\PYG{n}{args}\PYG{p}{)} \PYG{k}{as} \PYG{n}{writer}\PYG{p}{:}
    \PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}sources}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{concepts} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{concept} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{conceptlists}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{concepts}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{idx} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0\PYGZcb{}}\PYG{l+s+s2}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}1\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{concept}\PYG{o}{.}\PYG{n}{number}\PYG{p}{,} \PYG{n}{slug}\PYG{p}{(}\PYG{n}{concept}\PYG{o}{.}\PYG{n}{gloss}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}concept}\PYG{p}{(}
                \PYG{n}{ID}\PYG{o}{=}\PYG{n}{idx}\PYG{p}{,}
                \PYG{n}{Name}\PYG{o}{=}\PYG{n}{concept}\PYG{o}{.}\PYG{n}{gloss}\PYG{p}{,}
                \PYG{n}{Concepticon\PYGZus{}ID}\PYG{o}{=}\PYG{n}{concept}\PYG{o}{.}\PYG{n}{concepticon\PYGZus{}id}\PYG{p}{,}
                \PYG{n}{Concepticon\PYGZus{}Gloss}\PYG{o}{=}\PYG{n}{concept}\PYG{o}{.}\PYG{n}{concepticon\PYGZus{}gloss}\PYG{p}{,}
                \PYG{p}{)}
        \PYG{n}{concepts}\PYG{p}{[}\PYG{n}{concept}\PYG{o}{.}\PYG{n}{concepticon\PYGZus{}id}\PYG{p}{]} \PYG{o}{=} \PYG{n}{idx}
    \PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{added concepts}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are writing the table \sphinxcode{\sphinxupquote{cldf/parameters.csv}}, which contains
additional information about meanings and links them to reference catalogues.
It is based on a concept list that we have specified in the metadata.json
file, when setting up the repository earlier.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{language} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{languages}\PYG{p}{:}
    \PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}language}\PYG{p}{(}
            \PYG{n}{ID}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hungarian}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{n}{Name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hungarian}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{n}{Glottocode}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hung1274}\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{p}{)}
\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{added languages}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this section, we are creating the file \sphinxcode{\sphinxupquote{cldf/language.csv}} by adding
language
IDs. Since this repository consists of only one language, it can be hard\sphinxhyphen{}coded
as “Hungarian”, together with its code in the \sphinxhref{https://glottolog.org/}{Glottolog} reference catalogue.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{language\PYGZus{}table} \PYG{o}{=} \PYG{n}{writer}\PYG{o}{.}\PYG{n}{cldf}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{LanguageTable}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

\PYG{k}{for} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{raw\PYGZus{}dir}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wordlist.tsv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dicts}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{try}\PYG{p}{:}
        \PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}forms\PYGZus{}from\PYGZus{}value}\PYG{p}{(}
            \PYG{n}{Local\PYGZus{}ID}\PYG{o}{=}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Language\PYGZus{}ID}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hungarian}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{n}{Parameter\PYGZus{}ID}\PYG{o}{=}\PYG{n}{concepts}\PYG{p}{[}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CONCEPTICON\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Value}\PYG{o}{=}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{FORM}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Meaning}\PYG{o}{=}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MEANING}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Entry\PYGZus{}ID}\PYG{o}{=}\PYG{n}{form2idx}\PYG{p}{[}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{FORM}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SENSE}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Sense\PYGZus{}ID}\PYG{o}{=}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SENSE\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Source}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{uesz}\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{p}{)}
    \PYG{k}{except} \PYG{n+ne}{KeyError}\PYG{p}{:}
        \PYG{k}{pass}

 \PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{FormTable: done}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are creating the file \sphinxcode{\sphinxupquote{cldf/forms.csv}}, which is created by looping
through the rows of \sphinxcode{\sphinxupquote{raw/wordlist.tsv}}. This file in turn is a filtered
version of \sphinxcode{\sphinxupquote{raw/Gerstner\sphinxhyphen{}2016\sphinxhyphen{}10176.tsv}}. The filtering process will be
explained in Part 2. The columns \sphinxcode{\sphinxupquote{Local\_ID}} \sphinxcode{\sphinxupquote{Value}} \sphinxcode{\sphinxupquote{Meaning}}
and \sphinxcode{\sphinxupquote{Sense\_ID}} are directly filled from the raw file, while the columns
\sphinxcode{\sphinxupquote{Parameter\_ID}} (foreign keys to \sphinxcode{\sphinxupquote{cldf/parameters.csv}}) and \sphinxcode{\sphinxupquote{Entry\_ID}}
(foreign keys to \sphinxcode{\sphinxupquote{cldf/entries.csv}}) are filled by accessing the information
stored in the dictionaries \sphinxcode{\sphinxupquote{concepts}} and \sphinxcode{\sphinxupquote{form2idx}} that we have created
a little earlier. The values in columns \sphinxcode{\sphinxupquote{Language\_ID}} and \sphinxcode{\sphinxupquote{Source}} are
always the same and can therefore be hard\sphinxhyphen{}coded.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cldf\PYGZus{}writer}\PYG{p}{(}\PYG{n}{args}\PYG{p}{,} \PYG{n}{cldf\PYGZus{}spec}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dictionary}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{clean}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)} \PYG{k}{as} \PYG{n}{writer}\PYG{p}{:}

    \PYG{n}{writer}\PYG{o}{.}\PYG{n}{cldf}\PYG{o}{.}\PYG{n}{add\PYGZus{}component}\PYG{p}{(}\PYG{n}{language\PYGZus{}table}\PYG{p}{)}

    \PYG{n}{writer}\PYG{o}{.}\PYG{n}{cldf}\PYG{o}{.}\PYG{n}{add\PYGZus{}columns}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{EntryTable}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Segments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datatype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Year}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datatype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{integer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Etymology}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datatype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loan}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datatype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{n}{rc}\PYG{p}{\PYGZob{}}\PYG{n}{HOWMANY}\PYG{p}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datatype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}

    \PYG{n}{writer}\PYG{o}{.}\PYG{n}{cldf}\PYG{o}{.}\PYG{n}{add\PYGZus{}columns}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SenseTable}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Spacy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{datatype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are summoning two types of tables: \sphinxhref{https://github.com/cldf/cldf/tree/master/components/entries}{EntryTable} and
\sphinxhref{https://github.com/cldf/cldf/tree/master/components/senses}{SenseTable}.
A list
of possible table types to choose from is listed in \sphinxhref{https://github.com/cldf/cldf/tree/master/components}{CLDF’s GitHub reposiotry}. We are adding
some extra columns to the default settings, which are needed for our specific
use\sphinxhyphen{}case. Namely columns \sphinxcode{\sphinxupquote{Segments}}, \sphinxcode{\sphinxupquote{Year}}, \sphinxcode{\sphinxupquote{Etymology}}, \sphinxcode{\sphinxupquote{Loan}}, and
\sphinxcode{\sphinxupquote{f"rc\{HOWMANY\}"}} to \sphinxcode{\sphinxupquote{EntryTable}} and \sphinxcode{\sphinxupquote{Spacy}} to \sphinxcode{\sphinxupquote{SenseTable}}.
The purpose of these columns will be clarified in the next paragraphs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{senses\PYGZus{}items} \PYG{o}{=} \PYG{n}{senses}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Checking word vectors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{j}\PYG{p}{,} \PYG{p}{(}\PYG{n}{sense}\PYG{p}{,} \PYG{n}{values}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}
        \PYG{n}{tqdm}\PYG{p}{(}\PYG{n}{senses\PYGZus{}items}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Checking word vectors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{p}{(}\PYG{n}{fidx}\PYG{p}{,} \PYG{n}{sense\PYGZus{}desc}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{values}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{vector} \PYG{o}{=} \PYG{n}{filter\PYGZus{}vectors}\PYG{p}{(}\PYG{n}{sense\PYGZus{}desc}\PYG{p}{)}
        \PYG{n}{writer}\PYG{o}{.}\PYG{n}{objects}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SenseTable}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{sense} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{i} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Entry\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{fidx}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Description}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{sense\PYGZus{}desc}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Spacy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{vector}
            \PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SenseTable: done}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are creating the file \sphinxcode{\sphinxupquote{cldf/senses.csv}} by looping through the
\sphinxcode{\sphinxupquote{senses}} object that we have created earlier. Each row of the column
\sphinxcode{\sphinxupquote{sense}} in the file \sphinxcode{\sphinxupquote{raw/Gerstner\sphinxhyphen{}2016\sphinxhyphen{}10176.tsv}} contains multiple
translations to English, separated by “, “ (comma\sphinxhyphen{}space).
Since it is best practice to
avoid complex data structures like lists in databases, each translation
will get its own row and a foreign key in the file \sphinxcode{\sphinxupquote{cldf/senses.csv}}.
Since the raw file contains roughly 10,000 rows and there are on average ca. 4
translations per entry, we end up with a \sphinxcode{\sphinxupquote{cldf/senses.csv}} table of ca.
40,000 rows.
Apart from the default columns \sphinxcode{\sphinxupquote{ID}} (the primary key), \sphinxcode{\sphinxupquote{Entry\_ID}}
(foreign keys pointing to \sphinxcode{\sphinxupquote{cldf/entries.csv}}) and \sphinxcode{\sphinxupquote{Description}}
(containing the single translations), there is one custom column that we
have added, namely \sphinxcode{\sphinxupquote{Spacy}}. Here we are removing unwanted characters and
checking whether the translation has a word\sphinxhyphen{}vector representation in the
\sphinxhref{https://pypi.org/project/spacy/}{Spacy} vector model that we have
downloaded in step 3 and loaded in the beginning of this script. This is
being done with the \sphinxcode{\sphinxupquote{filter\_vectors}} function that we have described
earlier in this section. A progressbar is printed to the console with
\sphinxcode{\sphinxupquote{tqdm}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{fidx}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{idxs}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{seg\PYGZus{}ipa} \PYG{o}{=} \PYG{n}{tokens2clusters}\PYG{p}{(}\PYG{n}{ipa2tokens}\PYG{p}{(}\PYG{n}{orth2ipa}\PYG{p}{(}\PYG{n}{clean1}\PYG{p}{(}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{form}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{writer}\PYG{o}{.}\PYG{n}{objects}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{EntryTable}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{fidx}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Language\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hungarian}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Headword}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{form}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Segments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{seg\PYGZus{}ipa}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Year}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Etymology}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{origin}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loan}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loan}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rc}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{HOWMANY}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{rc}\PYG{o}{.}\PYG{n}{reconstruct}\PYG{p}{(}\PYG{n}{seg\PYGZus{}ipa}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
        \PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{EntryTable: done}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In this final step we are creating the table \sphinxcode{\sphinxupquote{cldf/entries.csv}}, which
will serve as input for the loanword searching module of the loanpy library
later on. This
table contains the same amount of rows as \sphinxcode{\sphinxupquote{raw/Gerstner\sphinxhyphen{}2016\sphinxhyphen{}10176.tsv}}, so
it is unfiltered. The columns \sphinxcode{\sphinxupquote{Headword}} \sphinxcode{\sphinxupquote{Year}} \sphinxcode{\sphinxupquote{Etymology}} and \sphinxcode{\sphinxupquote{Loan}}
are directly taken from the raw file. The primary key \sphinxcode{\sphinxupquote{ID}} was
defined in the beginning of the \sphinxcode{\sphinxupquote{cmd\_makecldf}} method and was stored in the
\sphinxcode{\sphinxupquote{idxs}} object, through which we are looping at the moment. \sphinxcode{\sphinxupquote{Language\_ID}}
is the only foreign key, pointing to the one language in
\sphinxcode{\sphinxupquote{cldf/languages.csv}}. There are two columns that will contain processed
information. One is \sphinxcode{\sphinxupquote{Segments}}: These are IPA transcriptions from cleaned
data, tokenised and segmented. This
step has to be done using the \sphinxhref{https://pypi.org/project/epitran/}{epitran}
library, since pylexibank’s automatic orthographic transcription can only be
used in \sphinxcode{\sphinxupquote{cldf/forms.csv}}, which in our case contains only filtered data.
The second, and most important column for further analysis is \sphinxcode{\sphinxupquote{f"rc\{HOWMANY\}"}}.
\sphinxcode{\sphinxupquote{rc}} stands for “reconstruct” and HOWMANY for the number of guesses or false
positives per attempted reconstruction that we have defined in the beginning.
The reconstruction itself is a regular
expression, created by \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scapplier.Adrc.reconstruct}{loanpy.scapplier.Adrc.reconstruct}.
In the end, we print a message with our logger that the table was created
successfully.

\sphinxAtStartPar
This is how your console should approximately look like after the conversion:

\noindent\sphinxincludegraphics{{consoleoutput}.png}


\section{Step 5: Update readme}
\label{\detokenize{mkcldf:step-5-update-readme}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }gerstnerhungarian.update\PYGZus{}readme
\end{sphinxVerbatim}

\sphinxAtStartPar
Here’s what happens under the hood:

\phantomsection\label{\detokenize{mkcldf:module-gerstnerhungariancommands.update_readme}}\index{module@\spxentry{module}!gerstnerhungariancommands.update\_readme@\spxentry{gerstnerhungariancommands.update\_readme}}\index{gerstnerhungariancommands.update\_readme@\spxentry{gerstnerhungariancommands.update\_readme}!module@\spxentry{module}}
\sphinxAtStartPar
Update the statistics section of the readme by adding two badges and a bullet
informing about the total number of senses and the proportion of senses that
were covered in Spacy’s word vector model.
\index{run() (in module gerstnerhungariancommands.update\_readme)@\spxentry{run()}\spxextra{in module gerstnerhungariancommands.update\_readme}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkcldf:gerstnerhungariancommands.update_readme.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.update\_readme.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Open \sphinxcode{\sphinxupquote{cldf/senses.csv}}, and calculate the proportion of non\sphinxhyphen{}empty rows
in column \sphinxcode{\sphinxupquote{Description}}. Add this proportion in form of a badge to the
statistics section of the readme. Add another badge informing about the
version of Spacy used. Lastly, add a bullet informing about the total
amount of senses in \sphinxcode{\sphinxupquote{cldf/senses.csv}}, to which the percentage in the
badge refers to.

\end{fulllineitems}



\section{Step 6: Test with pytest\sphinxhyphen{}cldf whether the dataset conforms to CLDF}
\label{\detokenize{mkcldf:step-6-test-with-pytest-cldf-whether-the-dataset-conforms-to-cldf}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip\PYG{+w}{ }install\PYG{+w}{ }pytest\PYGZhy{}cldf
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cldf\PYGZhy{}metadata\PYG{o}{=}cldf/cldf\PYGZhy{}metadata.json\PYG{+w}{ }test.py
\end{sphinxVerbatim}

\sphinxAtStartPar
This runs one test with pytest that verifies that the data in the folder
\sphinxcode{\sphinxupquote{cldf}} conforms to the CLDF\sphinxhyphen{}standard. If it says “1 passed” on your console
the conversion was successful and you can click on the \sphinxcode{\sphinxupquote{Next}} button
to see how to filter the converted data.

\sphinxstepscope


\chapter{Part 2: Filtering}
\label{\detokenize{filter:part-2-filtering}}\label{\detokenize{filter::doc}}
\sphinxAtStartPar
Different analyses require different portions of the original data
\sphinxcode{\sphinxupquote{raw/Gerstner\sphinxhyphen{}2016\sphinxhyphen{}10176.tsv}}. Some tasks in computational historical
linguistics require words that can be mapped to core concepts of a language’s
vocabulary. For these type of tasks, one has to try to map the
meanings of a word to a single concept in a catalogue. Here,
this task is achieved with following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }gerstnerhungarian.map
\end{sphinxVerbatim}

\sphinxAtStartPar
This command creates the file \sphinxcode{\sphinxupquote{raw/wordlist.tsv}} from \sphinxcode{\sphinxupquote{cldf/senses.csv}}.
A file that contains less than half the amount of rows than the ca. 10,000
row long \sphinxcode{\sphinxupquote{raw/Gerstner\sphinxhyphen{}2016\sphinxhyphen{}10176.tsv}}.

\phantomsection\label{\detokenize{filter:module-gerstnerhungariancommands.map}}\index{module@\spxentry{module}!gerstnerhungariancommands.map@\spxentry{gerstnerhungariancommands.map}}\index{gerstnerhungariancommands.map@\spxentry{gerstnerhungariancommands.map}!module@\spxentry{module}}
\sphinxAtStartPar
Map concepts to concepticon and make a wordlist.
\index{register() (in module gerstnerhungariancommands.map)@\spxentry{register()}\spxextra{in module gerstnerhungariancommands.map}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{filter:gerstnerhungariancommands.map.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.map.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register optional arguments

\end{fulllineitems}

\index{run() (in module gerstnerhungariancommands.map)@\spxentry{run()}\spxextra{in module gerstnerhungariancommands.map}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{filter:gerstnerhungariancommands.map.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.map.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Filter raw data according to whether it occurs in the concept list called
“Dellert\sphinxhyphen{}2018\sphinxhyphen{}1016”, which was specified in \sphinxcode{\sphinxupquote{metadata.json}} after
creating the repository with the \sphinxcode{\sphinxupquote{cldfbench new}} command. The concept
list is fetched through concepticon’s API. Note that this script only
functions after a conversion to CLDF was already successful. This is
because the input is \sphinxcode{\sphinxupquote{cldf/senses.csv}}, which was created during the
cldf\sphinxhyphen{}conversion. The senses in that table are automatically mapped to
concepticon with the \sphinxhref{https://pypi.org/project/pysem/}{pyesm} package.
The output file is written to \sphinxcode{\sphinxupquote{raw/wordlist.tsv}}.

\end{fulllineitems}


\sphinxAtStartPar
To search for old loanwords with loanpy, we will also need to filter the
original input data, but according to different criteria. One filter criterion
is the year of first appearance in a written source. Words that start
appearing late are unlikely to be old. With this command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }gerstnerhungarian.filter
\end{sphinxVerbatim}

\sphinxAtStartPar
the rows of \sphinxcode{\sphinxupquote{cldf/entries.csv}} are filtered according to their year of first
appearance, which is stored in the column \sphinxcode{\sphinxupquote{Year}}. The optimal cut\sphinxhyphen{}off year
is calculated automatically by looking at the oldest layers of Hungarian:
words of Proto\sphinxhyphen{}Uralic, Proto\sphinxhyphen{}Finno\sphinxhyphen{}Ugric, Proto\sphinxhyphen{}Ugric, Turkic, unknown, and
and uncertain origin. The optimal cut\sphinxhyphen{}off year in our case is 1416, which
is reflected in the output file name \sphinxcode{\sphinxupquote{loanpy/hun1416.tsv}}. For more details
on how the optimal cut\sphinxhyphen{}off year is calculated see the documentation of
\sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.find\_optimal\_year\_cutoff}{loanpy.utility’s find\_optimal\_year\_cutoff}
function. Filtering by year of first appearance is possible with the \sphinxhyphen{}y flag,
by etymological origin with the \sphinxhyphen{}o flag, and with the \sphinxhyphen{}a flag one can indicate
whether entries with missing data should be included. For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }gerstnerhungarian.filter\PYG{+w}{ }\PYGZhy{}y\PYG{+w}{ }\PYG{l+m}{1600}\PYG{+w}{ }\PYGZhy{}o\PYG{+w}{ }SlavicTurkic\PYG{+w}{ }\PYGZhy{}a
\end{sphinxVerbatim}

\sphinxAtStartPar
will include words of Slavic or Turkic origin that appeared before 1600 in a
written text while words with no data about year or etymological origin
will still be included.

\phantomsection\label{\detokenize{filter:module-gerstnerhungariancommands.filter}}\index{module@\spxentry{module}!gerstnerhungariancommands.filter@\spxentry{gerstnerhungariancommands.filter}}\index{gerstnerhungariancommands.filter@\spxentry{gerstnerhungariancommands.filter}!module@\spxentry{module}}
\sphinxAtStartPar
Filter \sphinxcode{\sphinxupquote{cldf/entries.csv}} according to year or first appearance and
etymological origin. Run \sphinxcode{\sphinxupquote{cldfbench gerstnerhungarian.filter \sphinxhyphen{}\sphinxhyphen{}help}}
for help.
\index{add\_all\_etymologies() (in module gerstnerhungariancommands.filter)@\spxentry{add\_all\_etymologies()}\spxextra{in module gerstnerhungariancommands.filter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{filter:gerstnerhungariancommands.filter.add_all_etymologies}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.filter.}}\sphinxbfcode{\sphinxupquote{add\_all\_etymologies}}}{\sphinxparam{\DUrole{n,n}{entries}}, \sphinxparam{\DUrole{n,n}{h}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Loop through column \sphinxcode{\sphinxupquote{Etymology}} in \sphinxcode{\sphinxupquote{entries.csv}} and return a set
of all possible etymologies.

\end{fulllineitems}

\index{find\_empty() (in module gerstnerhungariancommands.filter)@\spxentry{find\_empty()}\spxextra{in module gerstnerhungariancommands.filter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{filter:gerstnerhungariancommands.filter.find_empty}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.filter.}}\sphinxbfcode{\sphinxupquote{find\_empty}}}{\sphinxparam{\DUrole{n,n}{senses\_file}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Loop through \sphinxcode{\sphinxupquote{cldf/senses.csv}} and add \sphinxcode{\sphinxupquote{Entry\_ID}} to list if \sphinxcode{\sphinxupquote{Spacy}}
empty.

\end{fulllineitems}

\index{register() (in module gerstnerhungariancommands.filter)@\spxentry{register()}\spxextra{in module gerstnerhungariancommands.filter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{filter:gerstnerhungariancommands.filter.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.filter.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register arguments for the run\sphinxhyphen{}function. Three optional flags: \sphinxhyphen{}y \sphinxhyphen{}o \sphinxhyphen{}a.
An integer after \sphinxhyphen{}y indicates the year above which words are filtered out.
The string after \sphinxhyphen{}o indicates the etymological origins the words in the
output should belong to. If \sphinxhyphen{}a is toggled on, words with missing data
will be included in the output.

\end{fulllineitems}

\index{run() (in module gerstnerhungariancommands.filter)@\spxentry{run()}\spxextra{in module gerstnerhungariancommands.filter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{filter:gerstnerhungariancommands.filter.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gerstnerhungariancommands.filter.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read \sphinxcode{\sphinxupquote{cldf/entries.csv}}

\item {} 
\sphinxAtStartPar
Filter according to year of first appearance, etymological origin,
and missing data.

\item {} 
\sphinxAtStartPar
write results to \sphinxcode{\sphinxupquote{loanpy/hun\{year\}\{etymology\}.tsv}}

\end{enumerate}

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{g}
\item\relax\sphinxstyleindexentry{gerstnerhungariancommands.\_\_init\_\_}\sphinxstyleindexpageref{TL;DR:\detokenize{module-gerstnerhungariancommands.__init__}}
\item\relax\sphinxstyleindexentry{gerstnerhungariancommands.filter}\sphinxstyleindexpageref{filter:\detokenize{module-gerstnerhungariancommands.filter}}
\item\relax\sphinxstyleindexentry{gerstnerhungariancommands.map}\sphinxstyleindexpageref{filter:\detokenize{module-gerstnerhungariancommands.map}}
\item\relax\sphinxstyleindexentry{gerstnerhungariancommands.update\_readme}\sphinxstyleindexpageref{mkcldf:\detokenize{module-gerstnerhungariancommands.update_readme}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}